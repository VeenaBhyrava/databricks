{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9a7010e-3705-402a-87fb-ea4bfad3039c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class batch_count:\n",
    "    def __init__(self):\n",
    "        self.base_dir=\"/FileStore/project1\"\n",
    "    \n",
    "    def read_data(self):\n",
    "        return spark.read.format(\"text\").option(\"linSep\",\".\").load(f\"{self.base_dir}/data/text\")\n",
    "    # spark.read.format(\"text\").option(\"linSep\",\".\").load(f\"{self.base_dir}/data/text\")\n",
    "    \n",
    "    def clean_data(self,raw_df):\n",
    "        from pyspark.sql.functions import trim,lower,split,explode,col\n",
    "        lines=raw_df.select(explode(split(raw_df.value,\" \")).alias(\"words\"))\n",
    "        return (lines.select(lower(trim(\"words\")).alias(\"words\")).where(col(\"words\").isNotNull() & col(\"words\").rlike(\"[a-z]\")))\n",
    "    \n",
    "    def countRes(self,clean_df):\n",
    "        from pyspark.sql.functions import count\n",
    "        return (clean_df.groupBy(\"words\").count())\n",
    "\n",
    "    def writeResToDelta(self,df):\n",
    "        (df.write\\\n",
    "                .format(\"delta\")\\\n",
    "                .mode(\"overwrite\")\\\n",
    "                .saveAsTable(\"word_count_table\"))\n",
    "        \n",
    "    def computeBatch(self):\n",
    "        print(f\"\\tExecuting word count...\",end='')\n",
    "        raw_df=self.read_data()\n",
    "        clean_df=self.clean_data(raw_df)\n",
    "        df=self.countRes(clean_df)\n",
    "        self.writeResToDelta(df)\n",
    "        print(\"Done!\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f587861-2a6a-4412-8bda-0b2fbf25eb99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class stream_count:\n",
    "    def __init__(self):\n",
    "        self.base_dir=\"/FileStore/project1\"\n",
    "    \n",
    "    def read_data(self):\n",
    "        return spark.readStream.format(\"text\").option(\"linSep\",\".\").load(f\"{self.base_dir}/data/text\")\n",
    "    # spark.read.format(\"text\").option(\"linSep\",\".\").load(f\"{self.base_dir}/data/text\")\n",
    "    \n",
    "    def clean_data(self,raw_df):\n",
    "        from pyspark.sql.functions import trim,lower,split,explode,col\n",
    "        lines=raw_df.select(explode(split(raw_df.value,\" \")).alias(\"words\"))\n",
    "        return (lines.select(lower(trim(\"words\")).alias(\"words\")).where(col(\"words\").isNotNull() & col(\"words\").rlike(\"[a-z]\")))\n",
    "    \n",
    "    def countRes(self,clean_df):\n",
    "        from pyspark.sql.functions import count\n",
    "        return (clean_df.groupBy(\"words\").count())\n",
    "\n",
    "    def writeResToDelta(self,df):\n",
    "        return (df.writeStream\\\n",
    "                .format(\"delta\")\\\n",
    "                .option(\"checkpointLocation\",f\"{self.base_dir}/checkpoint/word_count\")\\\n",
    "                .outputMode(\"complete\")\\\n",
    "                .toTable(\"word_count_table\"))\n",
    "        \n",
    "    def computeStream(self):\n",
    "        print(f\"\\tExecuting word count...\",end='')\n",
    "        raw_df=self.read_data()\n",
    "        clean_df=self.clean_data(raw_df)\n",
    "        df=self.countRes(clean_df)\n",
    "        sQuery=self.writeResToDelta(df)\n",
    "        print(\"Done!\\n\")\n",
    "        return sQuery\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00cf7f4e-4f0a-41c5-a882-b17d6dc5c8b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.mkdirs(\"/FileStore/project1/datasets\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 908016621782458,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01-wordcount",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
